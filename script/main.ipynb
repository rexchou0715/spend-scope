{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee8aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transaction_keywords import expense_categories, skip_city_country_keywords, income_categories\n",
    "from api_key import API1\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from time import sleep\n",
    "from collections import defaultdict\n",
    "from rapidfuzz import fuzz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d43275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBIT_PATH = \"../data/rules/rules_debit.json\"\n",
    "CREDIT_PATH = \"../data/rules/rules_credit.json\"\n",
    "\n",
    "# Store the rules\n",
    "def save_rules(rules_dict, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rules_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Read the rules\n",
    "def load_rules(path):\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        print(\"‚ö†Ô∏è Rules file missing or empty. Starting fresh.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ùå Invalid JSON format in rules file. Starting fresh.\")\n",
    "        return {}\n",
    "    \n",
    "\n",
    "def save_pending(credit_batch, debit_batch, path=\"pending_batches.json\"):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"credit_batch\": credit_batch,\n",
    "            \"debit_batch\": debit_batch\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_pending(path=\"pending_batches.json\"):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            return data.get(\"credit_batch\", []), data.get(\"debit_batch\", [])\n",
    "    except FileNotFoundError:\n",
    "        return [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe714775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Normalize Texts\n",
    "def normalize_keyword(name: str) -> str:\n",
    "    # Turn to lowercase, remove special character and extra white space\n",
    "    name_clean = name.lower()\n",
    "    name_clean = re.sub(r\"[^a-z0-9\\s]\", \"\", name_clean)  # preserve alphbat, number and some white space\n",
    "    name_clean = re.sub(r\"\\s+\", \" \", name_clean)  # remove additional space\n",
    "    return name_clean.strip()\n",
    "\n",
    "# Step 3 - Fuzzy Match\n",
    "\n",
    "def fuzzy_match_transaction(name_normalized, rules_dict, threshold=85):\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    best_keyword = None\n",
    "    matched_entry = None\n",
    "\n",
    "    for keyword, entries in rules_dict.items():\n",
    "        for entry in entries:\n",
    "            original_names = entry[\"Original Name\"]\n",
    "            original_names = original_names if isinstance(original_names, list) else [original_names]\n",
    "\n",
    "            for original_name in original_names:\n",
    "                name_clean = normalize_keyword(original_name)\n",
    "                score = fuzz.partial_ratio(name_normalized, name_clean)\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_match = original_name\n",
    "                    best_keyword = keyword\n",
    "                    matched_entry = entry\n",
    "\n",
    "    if best_score >= threshold:\n",
    "        return {\n",
    "            \"matched_keyword\": best_keyword,\n",
    "            \"matched_original_name\": best_match,\n",
    "            \"score\": best_score,\n",
    "            \"matched_entry\": matched_entry\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "\n",
    "# Step 4 Assign a cateogry\n",
    "def assign_rule_to_row(df, idx, match_result, rules_dict):\n",
    "    keyword = match_result[\"matched_keyword\"]\n",
    "    matched_name = match_result[\"matched_original_name\"]\n",
    "    name_norm = normalize_keyword(df.loc[idx, \"Name\"])\n",
    "\n",
    "    \n",
    "\n",
    "    entries = rules_dict.get(keyword, [])\n",
    "    selected_entry = None\n",
    "\n",
    "    for entry in entries:\n",
    "        cities = entry[\"City\"] if isinstance(entry[\"City\"], list) else [entry[\"City\"]]\n",
    "        for city in cities:\n",
    "            if city and city.lower() in name_norm:\n",
    "                selected_entry = entry\n",
    "                break\n",
    "        if selected_entry:\n",
    "            break\n",
    "\n",
    "    # fallback: first entry\n",
    "    if not selected_entry and entries:\n",
    "        selected_entry = entries[0]\n",
    "\n",
    "    if selected_entry:\n",
    "        df.loc[idx, \"Category\"] = selected_entry[\"Category\"]\n",
    "        df.loc[idx, \"Subcategory\"] = selected_entry.get(\"Subcategory\")\n",
    "        df.loc[idx, \"Keyword\"] = keyword\n",
    "        df.loc[idx, \"Match Type\"] = \"Fuzzy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28dd3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_keyword_rules(target, response_dict, rules_dict):\n",
    "    for keyword in response_dict[\"Keyword\"]:\n",
    "        key = keyword.strip().lower()\n",
    "        new_entry = {\n",
    "            \"Category\": response_dict[\"Category\"],\n",
    "            \"Subcategory\": response_dict.get(\"Subcategory\"),\n",
    "            \"Country\": [response_dict[\"Country\"]],\n",
    "            \"City\": [response_dict[\"City\"]],\n",
    "            \"Original Name\": [target],\n",
    "            \"Flags\":{\n",
    "                \"lock_location\": False,\n",
    "                \"manual_check\": False\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if key not in rules_dict:\n",
    "            rules_dict[key] = [new_entry]\n",
    "            print(f\"‚úÖ Added new keyword '{key}' with first entry.\")\n",
    "        else:\n",
    "            merged = False\n",
    "            for entry in rules_dict[key]:\n",
    "                if (\n",
    "                    entry[\"Category\"] == new_entry[\"Category\"] and\n",
    "                    entry[\"Subcategory\"] == new_entry[\"Subcategory\"]\n",
    "                ):\n",
    "                    flags = entry.get(\"Flags\", {})\n",
    "\n",
    "                    if not flags.get(\"manual_check\", False):\n",
    "                        if target not in entry[\"Original Name\"]:\n",
    "                            entry[\"Original Name\"].append(target)\n",
    "\n",
    "                    if not flags.get(\"lock_location\", False):\n",
    "                        if response_dict[\"Country\"] not in entry[\"Country\"]:\n",
    "                            entry[\"Country\"].append(response_dict[\"Country\"])\n",
    "                        if response_dict[\"City\"] not in entry[\"City\"]:\n",
    "                            entry[\"City\"].append(response_dict[\"City\"])\n",
    "\n",
    "                    print(f\"üîÅ Merged into existing keyword entry: {key}\")\n",
    "                    merged = True\n",
    "                    break\n",
    "\n",
    "            if not merged:\n",
    "                rules_dict[key].append(new_entry)\n",
    "                print(f\"‚ûï Added new variant entry under keyword: {key}\")\n",
    "\n",
    "def merge_entries(entries):\n",
    "    grouped = defaultdict(lambda: {\n",
    "        \"Original Name\": [],\n",
    "        \"Category\": None,\n",
    "        \"Subcategory\": None,\n",
    "        \"Country\": [],\n",
    "        \"City\": []\n",
    "    })\n",
    "\n",
    "    for entry in entries:\n",
    "        key = (\n",
    "            entry[\"Category\"],\n",
    "            entry[\"Subcategory\"]\n",
    "        )\n",
    "\n",
    "        group = grouped[key]\n",
    "\n",
    "        # Set static fields\n",
    "        group[\"Category\"] = entry[\"Category\"]\n",
    "        group[\"Subcategory\"] = entry[\"Subcategory\"]\n",
    "\n",
    "        # Original Name\n",
    "        orig_name = entry[\"Original Name\"]\n",
    "        if orig_name not in group[\"Original Name\"]:\n",
    "            group[\"Original Name\"].append(orig_name)\n",
    "\n",
    "        # Country\n",
    "        country = entry[\"Country\"]\n",
    "        if country not in group[\"Country\"]:\n",
    "            group[\"Country\"].append(country)\n",
    "\n",
    "        # City\n",
    "        city = entry[\"City\"]\n",
    "        if city not in group[\"City\"]:\n",
    "            group[\"City\"].append(city)\n",
    "\n",
    "    return list(grouped.values())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d86bb62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_debit = load_rules(\"../data/rules/rules_debit.json\")\n",
    "rules_credit = load_rules(\"../data/rules/rules_credit.json\")\n",
    "credit_batch, debit_batch = load_pending()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b528c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_api(transaction_list, current_API, expense_categories, keyword_rules, is_credit=True):\n",
    "    client = genai.Client(api_key=current_API)\n",
    "    credit_note = (\n",
    "        \"These transactions are **credits** (money received). \"\n",
    "        \"Classify them carefully ‚Äî common categories include salary, refunds, reimbursements, and transfers.\"\n",
    "        if is_credit else\n",
    "        \"These transactions are **debits** (money spent). \"\n",
    "        \"Classify them according to your schema ‚Äî avoid assigning refund or income categories.\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in payment services and transaction classification.\n",
    "\n",
    "Here is my classification schema:\n",
    "{expense_categories}\n",
    "\n",
    "If a transaction contains the word \"Tikkie\", label it as \"Miscellaneous\".\n",
    "\n",
    "{credit_note}\n",
    "\n",
    "Please classify the following transactions. For each transaction:\n",
    "\n",
    "1. Determine the category and subcategory based only on my schema (do not invent new words).\n",
    "2. Identify the most likely country and city this business is located in (no abbreviations).\n",
    "3. Provide one most representative keyword (e.g., brand name, but no city/country or invented names).\n",
    "\n",
    "Return your answer in strict JSON format like the following:\n",
    "\n",
    "{{\n",
    "  \"Transaction Name\": {{\n",
    "    \"Category\": \"...\",\n",
    "    \"Subcategory\": \"...\",\n",
    "    \"Country\": \"...\",\n",
    "    \"City\": \"...\",\n",
    "    \"Keyword\": [\"...\"]\n",
    "  }},\n",
    "  ...\n",
    "}}\n",
    "\n",
    "Transaction list:\n",
    "{json.dumps(transaction_list, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            max_output_tokens=2000,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cleaned_text = response.text.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "\n",
    "    try:\n",
    "        data = json.loads(cleaned_text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"JSON parsing failed. Raw output:\")\n",
    "        print(cleaned_text)\n",
    "        return {}\n",
    "\n",
    "    for transaction, details in data.items():\n",
    "        add_to_keyword_rules(transaction, details, keyword_rules)\n",
    "\n",
    "    if is_credit:\n",
    "        save_rules(keyword_rules, CREDIT_PATH)\n",
    "    else:\n",
    "        save_rules(keyword_rules, DEBIT_PATH)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4731c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Processing remaining debit batch: [150, 174, 420, 424, 480, 538, 587, 617, 645, 1102, 1291, 1358, 1405]\n",
      "üîÅ Merged into existing keyword entry: ov\n",
      "üîÅ Merged into existing keyword entry: ov\n",
      "üîÅ Merged into existing keyword entry: cili pizza\n",
      "üîÅ Merged into existing keyword entry: kruidvat\n",
      "üîÅ Merged into existing keyword entry: ov\n",
      "üîÅ Merged into existing keyword entry: jumbo\n",
      "üîÅ Merged into existing keyword entry: ov\n",
      "üîÅ Merged into existing keyword entry: olympos\n",
      "üîÅ Merged into existing keyword entry: ov\n",
      "üîÅ Merged into existing keyword entry: ov\n",
      "üîÅ Merged into existing keyword entry: cili pizza\n",
      "üîÅ Merged into existing keyword entry: kruidvat\n",
      "üîÅ Merged into existing keyword entry: ov\n",
      "üîÅ Merged into existing keyword entry: jumbo\n",
      "üîÅ Merged into existing keyword entry: ov\n",
      "üîÅ Merged into existing keyword entry: olympos\n",
      "‚úÖ Debit batch cleared.\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "df_previous = pd.read_csv(\"../data/processed/cleaned_transactions_previous.csv\")\n",
    "df_ing = pd.read_csv(\"../data/raw/ing_bank_statement_sample_2.csv\")\n",
    "df_revolut = pd.read_csv(\"../data/raw/revolut_bank_statement_sample_2.csv\")\n",
    "\n",
    "\n",
    "# Clean the data\n",
    "## ING\n",
    "\"\"\" Column information\n",
    "\n",
    "    'Date',                -> Year/ Month/ Day\n",
    "    'Name / Description',  -> Name\n",
    "    'Account',             -> Remove\n",
    "    'Counterparty',        -> Remove\n",
    "    'Code',                -> Remove\n",
    "    'Debit/credit',        -> Debit/Credit -> Positive/Negative\n",
    "    'Amount (EUR)',        -> Keep\n",
    "    'Transaction type',    -> Remove\n",
    "    'Notifications'        -> Remove\n",
    "\"\"\"\n",
    "col_for_ING = [\"Date\", \"Name / Description\", \"Debit/credit\", \"Amount (EUR)\"]\n",
    "col_rename_dict_ING = {\n",
    "    \"Name / Description\": \"Name\",\n",
    "    \"Debit/credit\": \"IsCredit\",\n",
    "    \"Amount (EUR)\": \"Amount\"\n",
    "}\n",
    "\n",
    "# Extract only the needed columns, and rename them so that it can be merged with other dataframe\n",
    "df_ing = (\n",
    "    df_ing[col_for_ING]\n",
    "    .rename(columns=col_rename_dict_ING)\n",
    ")\n",
    "\n",
    "## Revlout\n",
    "\"\"\" Column information\n",
    "\n",
    "    'Type',           -> Remove\n",
    "    'Product',        -> Remove\n",
    "    'Started Date',   -> Remove\n",
    "    'Completed Date', -> Remove\n",
    "    'Description',    -> Name\n",
    "    'Amount',         -> Keep\n",
    "    'Fee',            -> Remove\n",
    "    'Currency',       -> Remove (So far only Euro)\n",
    "    'State',          -> Remove\n",
    "    'Balance'         -> Remove\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "col_for_revolut = [\"Description\", \"Amount\", \"Completed Date\"]\n",
    "col_rename_dict_Revlout = {\n",
    "    \"Description\": \"Name\",\n",
    "    \"Completed Date\": \"Date\"\n",
    "}\n",
    "\n",
    "df_revolut = (\n",
    "    df_revolut[col_for_revolut]\n",
    "    .rename(columns=col_rename_dict_Revlout)\n",
    ")\n",
    "\n",
    "## ING, Revlout (Check if the columns matched)\n",
    "\n",
    "# Transform the data\n",
    "\n",
    "## ING\n",
    "### Date -> Year/ Month/ Day\n",
    "df_ing[\"Date\"] = pd.to_datetime(df_ing[\"Date\"], format=\"%Y%m%d\")\n",
    "df_ing[\"Year\"] = df_ing[\"Date\"].dt.year\n",
    "df_ing[\"Month\"] = df_ing[\"Date\"].dt.month\n",
    "df_ing[\"Day\"] = df_ing[\"Date\"].dt.day\n",
    "### Amount -> 100,00 => 100.00\n",
    "df_ing[\"Amount\"] = df_ing[\"Amount\"].apply(\n",
    "    lambda row: float(row.replace(\",\", \".\")))\n",
    "### Amount + isCredit -> Amount (Positive/Negative)\n",
    "df_ing[\"Amount\"] = df_ing.apply(\n",
    "    lambda row: -row[\"Amount\"] if row[\"IsCredit\"] == \"Debit\" else row[\"Amount\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "### Create Source column -> Indiciate where the data is from\n",
    "df_ing[\"Source\"] = \"ING\"\n",
    "\n",
    "## Revlout\n",
    "\n",
    "### Date -> Year/ Month/ Day\n",
    "df_revolut[\"Date\"] = pd.to_datetime(df_revolut[\"Date\"])\n",
    "df_revolut[\"Year\"] = df_revolut[\"Date\"].dt.year.astype(\"Int32\") # Int 32 allows  Na\n",
    "df_revolut[\"Month\"] = df_revolut[\"Date\"].dt.month.astype(\"Int32\")\n",
    "df_revolut[\"Day\"] = df_revolut[\"Date\"].dt.day.astype(\"Int32\")\n",
    "\n",
    "### Create IsCredit column\n",
    "df_revolut[\"IsCredit\"] = df_revolut.apply(\n",
    "    lambda row: \"Debit\" if row[\"Amount\"] < 0 else \"Credit\",\n",
    "    axis=1\n",
    "    )\n",
    "\n",
    "### Create Source column -> Indiciate where the data is from\n",
    "df_revolut[\"Source\"] = \"Revolut\"\n",
    "\n",
    "\n",
    "# Reorder columns \n",
    "df_ing = df_ing[[\"Year\", \"Month\", \"Day\", \"Name\", \"Amount\", \"IsCredit\", \"Source\"]]\n",
    "df_revolut = df_revolut[[\"Year\", \"Month\", \"Day\", \"Name\", \"Amount\", \"IsCredit\", \"Source\"]]\n",
    "\n",
    "\n",
    "# Merge two data sources\n",
    "df = pd.concat([df_previous, df_ing, df_revolut], axis=0)\n",
    "df = df.sort_values(by=[\"Year\", \"Month\", \"Day\"], ascending=[False, True, True]).reset_index(drop=True)\n",
    "\n",
    "# Add new column\n",
    "df[\"Normalized Name\"] = df[\"Name\"].apply(normalize_keyword)\n",
    "\n",
    "\n",
    "df[df[\"Name\"] == \"Camelot Vastgoedbeheer BV\"]\n",
    "\n",
    "# Init Columns\n",
    "\n",
    "df[\"Category\"] = None\n",
    "df[\"Subcategory\"] = None\n",
    "df[\"Keyword\"] = None\n",
    "df[\"Match Type\"] = None\n",
    "\n",
    "\n",
    "# Init Pending List\n",
    "credit_batch = []\n",
    "debit_batch = []\n",
    "\n",
    "\n",
    "# # Classification\n",
    "for idx, row in df.iterrows():\n",
    "    is_credit = row[\"IsCredit\"] == \"Credit\"\n",
    "    name_norm = normalize_keyword(row[\"Name\"])\n",
    "    \n",
    "    if is_credit:\n",
    "        match = fuzzy_match_transaction(name_norm, rules_credit)\n",
    "        if match:\n",
    "            assign_rule_to_row(df, idx, match, rules_credit)\n",
    "        else:\n",
    "            credit_batch.append(idx)\n",
    "            df.loc[idx, \"Match Type\"] = \"Pending\"\n",
    "    else:\n",
    "        match = fuzzy_match_transaction(name_norm, rules_debit)\n",
    "        if match:\n",
    "            assign_rule_to_row(df, idx, match, rules_debit)\n",
    "        else:\n",
    "            debit_batch.append(idx)\n",
    "            df.loc[idx, \"Match Type\"] = \"Pending\"\n",
    "\n",
    "    # Trigger Gemini if enough new items\n",
    "    if len(credit_batch) >= 15:\n",
    "        print(\"Hit the quota! \", credit_batch)\n",
    "        batch = df.loc[credit_batch][\"Name\"].tolist()\n",
    "        result = gemini_api(batch, API1, income_categories, rules_credit, is_credit=True)\n",
    "        for name, info in result.items():\n",
    "            add_to_keyword_rules(name, info, rules_credit)\n",
    "        credit_batch.clear()\n",
    "        save_pending(credit_batch, debit_batch)\n",
    "        print(\"Clear!: \", credit_batch)\n",
    "        sleep(10)\n",
    "\n",
    "    if len(debit_batch) >= 15:\n",
    "        print(\"Hit the quota! \", debit_batch)\n",
    "        batch = df.loc[debit_batch][\"Name\"].tolist()\n",
    "        result = gemini_api(batch, API1, expense_categories, rules_debit, is_credit=False)\n",
    "        for name, info in result.items():\n",
    "            add_to_keyword_rules(name, info, rules_debit)\n",
    "        debit_batch.clear()\n",
    "        save_pending(credit_batch, debit_batch)\n",
    "        print(\"Clear!: \", debit_batch)\n",
    "        sleep(10)\n",
    "        \n",
    "if credit_batch:\n",
    "    print(\"üîÅ Processing remaining credit batch:\", credit_batch)\n",
    "    batch = df.loc[credit_batch][\"Name\"].tolist()\n",
    "    result = gemini_api(batch, API1, income_categories, rules_credit, is_credit=True)\n",
    "    for name, info in result.items():\n",
    "        add_to_keyword_rules(name, info, rules_credit)\n",
    "    credit_batch.clear()\n",
    "    save_pending(credit_batch, debit_batch)\n",
    "    print(\"‚úÖ Credit batch cleared.\")\n",
    "\n",
    "if debit_batch:\n",
    "    print(\"üîÅ Processing remaining debit batch:\", debit_batch)\n",
    "    batch = df.loc[debit_batch][\"Name\"].tolist()\n",
    "    result = gemini_api(batch, API1, expense_categories, rules_debit, is_credit=False)\n",
    "    for name, info in result.items():\n",
    "        add_to_keyword_rules(name, info, rules_debit)\n",
    "    debit_batch.clear()\n",
    "    save_pending(credit_batch, debit_batch)\n",
    "    print(\"‚úÖ Debit batch cleared.\")\n",
    "\n",
    "# Change the Type\n",
    "df[[\"Year\", \"Month\", \"Day\"]] = df[[\"Year\", \"Month\", \"Day\"]].astype(\"Int64\")\n",
    "\n",
    "# Output file\n",
    "df.to_csv(\"../data/processed/cleaned_transactions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b5d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
